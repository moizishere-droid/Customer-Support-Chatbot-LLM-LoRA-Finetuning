{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gewZQNviMUG"
      },
      "source": [
        "# ***IMPORTING***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtofJ1Eye3qh"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate datasets peft bitsandbytes evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6utrBKB5iQFN"
      },
      "source": [
        "# ***Libraries***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ag84neUUiTYl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMpHbSvcey44"
      },
      "source": [
        "# ***DATASET***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHLdsqGSepdY",
        "outputId": "c73fb92c-12b2-4dab-cd64-c76380a6a711"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'response': 'Over two million people died at Nagasaki and Hiroshima. Not exactly the hilarious fodder for making jokes.', 'prompt': 'It is called the bomb.', 'text': '### Instruction:\\nIt is called the bomb.\\n\\n### Response:\\nOver two million people died at Nagasaki and Hiroshima. Not exactly the hilarious fodder for making jokes.'}\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"alespalla/chatbot_instruction_prompts\")\n",
        "\n",
        "train_ds = ds[\"train\"].shuffle(seed=42).select(range(10000))   # first 10k examples\n",
        "test_ds = ds[\"test\"].shuffle(seed=42).select(range(2000))      # first 2k examples\n",
        "\n",
        "def format_example(e):\n",
        "    return {\n",
        "        \"text\": f\"### Instruction:\\n{e['prompt']}\\n\\n### Response:\\n{e['response']}\"\n",
        "    }\n",
        "\n",
        "train_ds = train_ds.map(format_example)\n",
        "test_ds = test_ds.map(format_example)\n",
        "\n",
        "print(train_ds[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7J1MroejvjD"
      },
      "source": [
        "# ***Tokenizer***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "72zXvQW-hcPa"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"gpt2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token # A: [101, 102, 2] , B: [101, 2,     2] --> 2 is <end> token to match the len\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuHeM7cjk6fA"
      },
      "source": [
        "# ***Load Model with 8-bit + LoRA Prep***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zrr-qFvQkxT6",
        "outputId": "0e5da110-6793-4ceb-be26-1e84704d3ddf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    load_in_8bit=True,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeRyWGFFmKHl"
      },
      "source": [
        "# ***Apply LoRA Configuration***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqxZR6ndmDhv",
        "outputId": "71b258e2-0a9e-4544-cd86-695fcf6aa91d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 294,912 || all params: 124,734,720 || trainable%: 0.2364\n"
          ]
        }
      ],
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"c_attn\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPINCz4Dm8WH"
      },
      "source": [
        "# ***Tokenization***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b456130f10584bd19b0263ea8a73e52f",
            "c262840822814a95b98767be495818fb",
            "d0026bb8f8f74af4bc9f8b606c467d0d",
            "2d0eedacbab044cc81ef21f906752501",
            "e47a7197c5c74618b09219d036a1e055",
            "9eabe3a0452a483dbb7cf2cf44f1f58b",
            "89b84082c07a44c298ef1fe0947be828",
            "44d4ff9e6fac4fe98785c9164b9fa42f",
            "a12bce5cc834490382cc42f24d632a1e",
            "a6b1a2f9a8e1400f8d9c494aa05fcfa0",
            "5c98e1b7feec49d6bdb60ee79544dc41"
          ]
        },
        "id": "2epZaKRBm0_v",
        "outputId": "2e4348c1-9443-494d-b65a-81bf4eac9af2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b456130f10584bd19b0263ea8a73e52f"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "def tokenize_fn(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=256  # shorter for faster training\n",
        "    )\n",
        "\n",
        "# Tokenize the subset datasets\n",
        "tokenized_train = train_ds.map(\n",
        "    tokenize_fn,\n",
        "    batched=True,\n",
        "    remove_columns=train_ds.column_names  # remove only columns in subset\n",
        ")\n",
        "\n",
        "tokenized_test = test_ds.map(\n",
        "    tokenize_fn,\n",
        "    batched=True,\n",
        "    remove_columns=test_ds.column_names\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-B-q47Vpon6N"
      },
      "source": [
        "# ***Data Collator***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "afuIiHPynWY2"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***evaluate***"
      ],
      "metadata": {
        "id": "-B2SE9pIbjDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "pHYCMqS-bmEy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UraAfNJwowqE"
      },
      "source": [
        "# ***Training Arguments***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NrGXsJcXoVAF"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/support-bot-gpt2\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    save_steps=1000,\n",
        "    logging_steps=50,\n",
        "    eval_steps=1000,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=False,\n",
        "    report_to=\"none\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAyRAe0kpK3i"
      },
      "source": [
        "# ***Trainer Setup***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "tXga6DeNpBI4"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,  # use tokenized subset\n",
        "    eval_dataset=tokenized_test,    # use tokenized subset\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yZ8rJu7BpPUb",
        "outputId": "5252ba11-5362-4121-9c06-eb8a0f6966c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1250/1250 13:34, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.505700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.512500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.446900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.456200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>2.467800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>2.464900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>2.490400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>2.492200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>2.421800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.443500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>2.453200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>2.482900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>2.462000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>2.440400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>2.417300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>2.455900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>2.416200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>2.430900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>2.459800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>2.429900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>2.404900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>2.436100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>2.415700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>2.422600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>2.400600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1250, training_loss=2.4492092041015625, metrics={'train_runtime': 814.9748, 'train_samples_per_second': 12.27, 'train_steps_per_second': 1.534, 'total_flos': 1310990008320000.0, 'train_loss': 2.4492092041015625, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***SAVING***"
      ],
      "metadata": {
        "id": "0xen5p4Jl549"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Xl7v55uUpT8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e3e53ad-08a0-41a3-b2b5-4c5426f9fb73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete! Model saved to: /content/support-bot-gpt2\n"
          ]
        }
      ],
      "source": [
        "OUTPUT_DIR = \"/content/support-bot-gpt2\"\n",
        "\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "print(\"Training complete! Model saved to:\", OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "VCKnl0pznhSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg0HhfFcqfZ5"
      },
      "source": [
        "# ***Inference***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "S7uSFdaFqgcJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e0151d7-5962-44ea-a15b-f8ca3ade5cc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“¥ Loading fine-tuned model for inference...\n",
            "ðŸŸ¢ Response:\n",
            " This is a simple process. You can use your credit or debit card to deposit the purchase price into a bank account (like Chase) for easy access, such as with Credit Cards CheckoutsÂ® services . These deposits are then returned at checkout if you leave without paying any money within 30 days. For more information, contact customerservices@mycreditcard.com on 01-8661 038402337 in order not miss out on one of the many great savings opportunities that comes from using my own online shopping app!\n",
            "\n",
            " - Your browser does some processing when visiting MyCreditCard.com; it may take up too much space temporarily so please be patient while checking all relevant facts about each transaction ! The amount received will depend on where exactly they came through but depends on which merchants' website accounts have been activated : PayPal , KwikPaymentServiceÂ®, WPPY Services Inc. & V\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Load your fine-tuned model\n",
        "OUTPUT_DIR = \"/content/support-bot-gpt2\"  # Folder where your model was saved\n",
        "\n",
        "print(\"ðŸ“¥ Loading fine-tuned model for inference...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n",
        "model = AutoModelForCausalLM.from_pretrained(OUTPUT_DIR, device_map=\"auto\", torch_dtype=torch.float16)\n",
        "model.eval()\n",
        "\n",
        "# Function for generating a response\n",
        "def generate_response(prompt, max_length=200, temperature=0.7, top_p=0.9):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "          **inputs,\n",
        "          max_length=200,\n",
        "          temperature=0.8,  # slightly higher randomness\n",
        "          top_p=0.95,       # nucleus sampling\n",
        "          do_sample=True,\n",
        "          pad_token_id=tokenizer.eos_token_id,\n",
        "          repetition_penalty=1.2  # discourages repetition\n",
        "      )\n",
        "\n",
        "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    # Remove prompt from output\n",
        "    return response[len(prompt):].strip()\n",
        "\n",
        "# Example usage\n",
        "prompt = \"Answer this customer support question professionally and helpfully:\\nHow do I return an item?\"\n",
        "response = generate_response(prompt)\n",
        "print(\"ðŸŸ¢ Response:\\n\", response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6bXNAQlgi_tr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b456130f10584bd19b0263ea8a73e52f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c262840822814a95b98767be495818fb",
              "IPY_MODEL_d0026bb8f8f74af4bc9f8b606c467d0d",
              "IPY_MODEL_2d0eedacbab044cc81ef21f906752501"
            ],
            "layout": "IPY_MODEL_e47a7197c5c74618b09219d036a1e055"
          }
        },
        "c262840822814a95b98767be495818fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9eabe3a0452a483dbb7cf2cf44f1f58b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_89b84082c07a44c298ef1fe0947be828",
            "value": "Map:â€‡100%"
          }
        },
        "d0026bb8f8f74af4bc9f8b606c467d0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44d4ff9e6fac4fe98785c9164b9fa42f",
            "max": 2000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a12bce5cc834490382cc42f24d632a1e",
            "value": 2000
          }
        },
        "2d0eedacbab044cc81ef21f906752501": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6b1a2f9a8e1400f8d9c494aa05fcfa0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5c98e1b7feec49d6bdb60ee79544dc41",
            "value": "â€‡2000/2000â€‡[00:01&lt;00:00,â€‡1589.51â€‡examples/s]"
          }
        },
        "e47a7197c5c74618b09219d036a1e055": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9eabe3a0452a483dbb7cf2cf44f1f58b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89b84082c07a44c298ef1fe0947be828": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "44d4ff9e6fac4fe98785c9164b9fa42f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a12bce5cc834490382cc42f24d632a1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a6b1a2f9a8e1400f8d9c494aa05fcfa0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c98e1b7feec49d6bdb60ee79544dc41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}